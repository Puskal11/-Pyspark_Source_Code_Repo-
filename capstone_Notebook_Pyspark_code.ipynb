{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfaf21c0-3ad6-4c63-918b-a53f4eb24839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIAXYKJVGSIDSCOYI43\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b2422a9-fad8-46fc-8bd7-f407c7657073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value Counts: {'Claim_Or_Rejected': 0, 'SUB_ID': 0, 'claim_amount': 0, 'claim_date': 0, 'claim_id': 0, 'claim_type': 0, 'disease_name': 0, 'patient_id': 0}\nClaim_Or_Rejected: 0\nSUB_ID: 0\nclaim_amount: 0\nclaim_date: 0\nclaim_id: 0\nclaim_type: 0\ndisease_name: 0\npatient_id: 0\nroot\n |-- Claim_Or_Rejected: string (nullable = true)\n |-- SUB_ID: string (nullable = true)\n |-- claim_amount: string (nullable = true)\n |-- claim_date: string (nullable = true)\n |-- claim_id: long (nullable = true)\n |-- claim_type: string (nullable = true)\n |-- disease_name: string (nullable = true)\n |-- patient_id: long (nullable = true)\n\nNull Value Counts: Disease_name {'SubGrpID': 0, ' Disease_ID': 0, 'Disease_name': 0}\nSubGrpID: 0\n Disease_ID: 0\nDisease_name: 0\nroot\n |-- SubGrpID: string (nullable = true)\n |--  Disease_ID: integer (nullable = true)\n |-- Disease_name: string (nullable = true)\n\nNull Value Counts:\nCountry: 0\npremium_written: 0\nzipcode: 0\nGrp_Id: 0\nGrp_Name: 0\nGrp_Type: 0\ncity: 0\nyear: 0\nroot\n |-- Country: string (nullable = true)\n |-- premium_written: integer (nullable = true)\n |-- zipcode: integer (nullable = true)\n |-- Grp_Id: string (nullable = true)\n |-- Grp_Name: string (nullable = true)\n |-- Grp_Type: string (nullable = true)\n |-- city: string (nullable = true)\n |-- year: integer (nullable = true)\n\nNull Value Counts:\nSubGrp_ID: 0\nGrp_Id: 0\nroot\n |-- SubGrp_ID: string (nullable = true)\n |-- Grp_Id: string (nullable = true)\n\nNull Value Counts:\nHospital_id: 0\nHospital_name: 0\ncity: 0\nstate: 0\ncountry: 0\nroot\n |-- Hospital_id: string (nullable = true)\n |-- Hospital_name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- state: string (nullable = true)\n |-- country: string (nullable = true)\n\nNull Value Counts:\nPatient_id: 0\nPatient_name: 17\npatient_gender: 0\npatient_birth_date: 0\npatient_phone: 2\ndisease_name: 0\ncity: 0\nhospital_id: 0\nroot\n |-- Patient_id: integer (nullable = true)\n |-- Patient_name: string (nullable = true)\n |-- patient_gender: string (nullable = true)\n |-- patient_birth_date: date (nullable = true)\n |-- patient_phone: string (nullable = true)\n |-- disease_name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- hospital_id: string (nullable = true)\n\nNull Value Counts: {'SubGrp_id': 0, 'SubGrp_Name': 0, 'Monthly_Premium': 0}\nSubGrp_id: 0\nSubGrp_Name: 0\nMonthly_Premium: 0\n+---------+-------------------+---------------+\n|SubGrp_id|        SubGrp_Name|Monthly_Premium|\n+---------+-------------------+---------------+\n|     S101|Deficiency Diseases|           3000|\n|     S102|           Accident|           1000|\n|     S103|         Physiology|           2000|\n|     S104|            Therapy|           1500|\n|     S105|          Allergies|           2300|\n|     S106|     Self inflicted|           1200|\n|     S107|             Cancer|           3200|\n|     S108| Infectious disease|           1500|\n|     S109|         Hereditary|           2000|\n|     S110|              Viral|           1000|\n+---------+-------------------+---------------+\n\nroot\n |-- SubGrp_id: string (nullable = true)\n |-- SubGrp_Name: string (nullable = true)\n |-- Monthly_Premium: integer (nullable = true)\n\nNull Value Counts:\nsub _id: 0\nfirst_name: 27\nlast_name: 0\nStreet: 0\nBirth_date: 0\nGender: 0\nPhone: 3\nCountry: 0\nCity: 0\nZip Code: 0\nSubgrp_id: 2\nElig_ind: 4\neff_date: 0\nterm_date: 0\nroot\n |-- sub _id: string (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- Street: string (nullable = true)\n |-- Birth_date: date (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Phone: string (nullable = true)\n |-- Country: string (nullable = true)\n |-- City: string (nullable = true)\n |-- Zip Code: integer (nullable = true)\n |-- Subgrp_id: string (nullable = true)\n |-- Elig_ind: string (nullable = true)\n |-- eff_date: date (nullable = true)\n |-- term_date: date (nullable = true)\n\n+----------+----------+-----------+-----------------+----------+------+--------------+-------+--------------------+--------+---------+--------+----------+----------+\n|   sub _id|first_name|  last_name|           Street|Birth_date|Gender|         Phone|Country|                City|Zip Code|Subgrp_id|Elig_ind|  eff_date| term_date|\n+----------+----------+-----------+-----------------+----------+------+--------------+-------+--------------------+--------+---------+--------+----------+----------+\n|SUBID10013|  Brahmvir|        Rai|        Shah Path|1991-11-11|  Male|+91 7316972612|  India|              Ambala|  249898|     S106|       N|2011-11-11|2020-05-23|\n|SUBID10012|Dharmadaas|     Tiwari|             Rama|1964-04-29|  Male|+91 6345482027|  India|Bhalswa Jahangir Pur|  430793|     S103|       N|1984-04-29|1988-02-07|\n|SUBID10011|        NA|Vishwakarma|      Rajagopalan|1955-01-22|Female|+91 4146391938|  India|           Ghaziabad|  337042|     S106|       N|1975-01-22|1978-11-02|\n|SUBID10018|Bhagavaana|      Kumar|    Kulkarni Zila|1935-09-16|Female|+91 6071745855|  India|        Shahjahanpur|  597276|     S101|       N|1955-09-16|1958-05-31|\n|SUBID10002|   Ujjawal|       Devi|      Mammen Zila|1980-04-16|  Male|+91 8547451606|  India|           Berhampur|  914455|     S106|       N|2000-04-16|2008-05-04|\n|SUBID10009|        NA|      Gupta|    Thakur Circle|1925-06-12|  Male|+91 1780763280|  India|           Bangalore|  957469|     S105|       Y|1945-06-12|1953-08-30|\n| SUBID1020|     Umang|  Srivastav|      Balay Chowk|1963-07-14|Female|+91 9485838770|  India|            Haridwar|  181692|     S109|       Y|1983-07-14|1986-01-15|\n| SUBID1010|        NA|     Divedi|          Dhillon|1976-02-03|  Male|+91 5586075345|  India|              Rajkot|  911319|     S102|       Y|1996-02-03|2002-01-27|\n|SUBID10000|    Harbir|Vishwakarma|       Baria Marg|1924-06-30|Female|+91 0112009318|  India|            Rourkela|  767058|     S107|       Y|1944-06-30|1954-01-14|\n|SUBID10014|        NA|  Srivastav|     Chandra Path|1981-01-25|Female|+91 2960004518|  India|Surendranagar Dud...|  111966|     S102|       N|2001-01-25|2005-07-13|\n|SUBID10016| Amritkala|  Srivastav|        Guha Path|1933-11-20|Female|+91 0537157280|  India|              Meerut|  863467|     S106|       Y|1953-11-20|1955-07-29|\n|SUBID10003|   Ballari|     Mishra|       Sahni Zila|1969-09-25|Female|+91 0106026841|  India|        Bihar Sharif|   91481|     S104|       N|1989-09-25|1995-06-05|\n|SUBID10005|     Atasi|       Seth|     Khatri Nagar|1967-10-02|  Male|+91 9747336855|  India|            Amravati|  229062|     S104|       Y|1987-10-02|1995-02-13|\n|SUBID10015|   Bhagvan|  Srivastav|            Edwin|1966-07-24|Female|+91 0297693485|  India|          Bhimavaram|  436513|     S105|       Y|1986-07-24|1988-02-04|\n|SUBID10019|        NA|     Maurya|     Sharaf Nagar|1924-11-09|Female|+91 8906694405|  India|            Jabalpur|  958538|     S104|       N|1944-11-09|1951-10-14|\n|SUBID10017|    Bandhu|       Seth|        Varughese|1996-10-15|  Male|+91 0695289163|  India|           Chinsurah|  136713|     S108|       N|2016-10-15|2018-06-08|\n|SUBID10001|  Brahmdev|     Sonkar|        Lala Marg|1948-12-20|Female|+91 1727749552|  India|        Tiruvottiyur|   34639|     S105|       Y|1968-12-20|1970-05-16|\n|SUBID10008|   Gurudas|      Gupta|      Sarin Nagar|1945-05-06|  Male|+91 1232859381|  India|           Kamarhati|  933226|     S103|       Y|1965-05-06|1970-09-16|\n|SUBID10004|   Devnath|  Srivastav|       Magar Zila|1946-05-01|Female|+91 1868774631|  India|         Bidhannagar|  531742|     S110|       N|1966-05-01|1970-12-09|\n| SUBID1006|    Manish|     Maurya|Swaminathan Chowk|1967-06-06|  Male|+91 4354294043|  India|              Panvel|  438733|     S109|      NA|1987-06-06|1995-03-21|\n+----------+----------+-----------+-----------------+----------+------+--------------+-------+--------------------+--------+---------+--------+----------+----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df1_claims = spark.read.json(\"s3://puskalawsbucket/capstone_puskal/claims.json\")\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df1_claims.columns:\n",
    "    null_counts[col_name] = df1_claims.filter(col(col_name).isNull()).count()\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\", null_counts)\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df1_claims.columns:\n",
    "    if df1_claims.schema[col_name].dataType == StringType():\n",
    "        df1_claims = df1_claims.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df1_claims_dupdropped= df1_claims.dropDuplicates()\n",
    "df1_claims_dupdropped.printSchema()\n",
    "\n",
    "\n",
    "df2_disease = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/disease.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df2_disease.columns:\n",
    "    null_counts[col_name] = df2_disease.filter(col(col_name).isNull()).count()\n",
    "\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\",col_name,null_counts)\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df2_disease.columns:\n",
    "    if df2_disease.schema[col_name].dataType == StringType():\n",
    "        df2_disease = df2_disease.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df2_disease_dupdropped=df2_disease.dropDuplicates()\n",
    "df2_disease_dupdropped.printSchema()\n",
    "\n",
    "df3_group = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/group.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df3_group.columns:\n",
    "    null_counts[col_name] = df3_group.filter(col(col_name).isNull()).count()\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\")\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df3_group.columns:\n",
    "    if df3_group.schema[col_name].dataType == StringType():\n",
    "        df3_group = df3_group.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df3_group_dupdropped=df3_group.dropDuplicates()\n",
    "df3_group_dupdropped.printSchema()\n",
    "\n",
    "df4_grpsubgrp = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/grpsubgrp.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df4_grpsubgrp.columns:\n",
    "    null_counts[col_name] = df4_grpsubgrp.filter(col(col_name).isNull()).count()\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\")\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df4_grpsubgrp.columns:\n",
    "    if df4_grpsubgrp.schema[col_name].dataType == StringType():\n",
    "        df4_grpsubgrp = df4_grpsubgrp.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df4_grpsubgrp_dupdropped=df4_grpsubgrp.dropDuplicates()\n",
    "df4_grpsubgrp_dupdropped.printSchema()\n",
    "\n",
    "df5_hospital = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/hospital.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df5_hospital.columns:\n",
    "    null_counts[col_name] = df5_hospital.filter(col(col_name).isNull()).count()\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\")\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df5_hospital.columns:\n",
    "    if df5_hospital.schema[col_name].dataType == StringType():\n",
    "        df5_hospital = df5_hospital.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df5_hospital_dupdropped=df5_hospital.dropDuplicates()\n",
    "df5_hospital_dupdropped.printSchema()\n",
    "\n",
    "df6_patient_record = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/Patient_records.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df6_patient_record.columns:\n",
    "    null_counts[col_name] = df6_patient_record.filter(col(col_name).isNull()).count()\n",
    "\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\")\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df6_patient_record.columns:\n",
    "    if df6_patient_record.schema[col_name].dataType == StringType():\n",
    "        df6_patient_record = df6_patient_record.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df6_patient_record_dupdropped=df6_patient_record.dropDuplicates()\n",
    "df6_patient_record_dupdropped.printSchema()\n",
    "\n",
    "df7_subgroup = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/subgroup.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column using dict\n",
    "null_counts = {}\n",
    "for col_name in df7_subgroup.columns:\n",
    "    null_counts[col_name] = df7_subgroup.filter(col(col_name).isNull()).count()\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\",null_counts) #debugging using print\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df7_subgroup.columns:\n",
    "    if df7_subgroup.schema[col_name].dataType == StringType():\n",
    "        df7_subgroup = df7_subgroup.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "        #Can also use inbuilt function named fillNA\n",
    "df7_subgroup.show() #debuggging using print\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "df7_subgroup_dupdropped=df7_subgroup.dropDuplicates()\n",
    "df7_subgroup_dupdropped.printSchema()\n",
    "\n",
    "df8_subscriber = spark.read.csv(\"s3://puskalawsbucket/capstone_puskal/subscriber.csv\",header='True', inferSchema='True')\n",
    "# Count null values for each column\n",
    "null_counts = {}\n",
    "for col_name in df8_subscriber.columns:\n",
    "    null_counts[col_name] = df8_subscriber.filter(col(col_name).isNull()).count()\n",
    "# Display null counts\n",
    "print(\"Null Value Counts:\")\n",
    "for col_name, count in null_counts.items():\n",
    "    print(f\"{col_name}: {count}\")\n",
    "# Replace null values with \"NA\" for string-type columns\n",
    "for col_name in df8_subscriber.columns:\n",
    "    if df8_subscriber.schema[col_name].dataType == StringType():\n",
    "        df8_subscriber = df8_subscriber.withColumn(col_name, when(col(col_name).isNull(), \"NA\").otherwise(col(col_name)))\n",
    "\n",
    "# Drop duplicate and Show DataFrame including the replaced values\n",
    "# df8_subscriber.createOrReplaceTempView(\"df1\")\n",
    "# df.createOrReplaceTempView(\"df1\")\n",
    "# df1 = spark.sql(\"\"\"\n",
    "#     SELECT \n",
    "#         `df`.`sub _id` as sub_id,\n",
    "#         `df`.`first_name` as first_name,\n",
    "#         `df`.`last_name` as last_name,\n",
    "#         `df`.`Street` as Street,\n",
    "#         `df`.`Birth_date` as Birth_date,\n",
    "#         `df`.`Gender` as Gender,\n",
    "#         `df`.`Phone` as Phone,\n",
    "#         `df`.`Country` as Country,\n",
    "#         `df`.`City` as City,\n",
    "#         `df`.`Zip Code` as Zip_Code,\n",
    "#         `df`.`Subgrp_id` as Subgrp_id,\n",
    "#         `df`.`Elig_ind` as Elig_ind,\n",
    "#         `df`.`eff_date` as eff_date,\n",
    "#         `df`.`term_date` as term_date\n",
    "#     FROM df\n",
    "# \"\"\")\n",
    "df8_subscriber_dupdropped=df8_subscriber.dropDuplicates()\n",
    "df8_subscriber_dupdropped.printSchema()\n",
    "df8_subscriber_dupdropped.show()\n",
    "\n",
    "# df1_claims_dupdropped.printSchema()\n",
    "# df2_disease_dupdropped.printSchema()\n",
    "# df3_group_dupdropped.printSchema()\n",
    "# df4_grpsubgrp_dupdropped.printSchema()\n",
    "# df5_hospital_dupdropped.printSchema()\n",
    "# df6_patient_record_dupdropped.printSchema()\n",
    "# df7_subgroup_dupdropped.printSchema()\n",
    "# df8_subscriber_dupdropped.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4060440c-6a83-4ccb-a9a0-33b6578519bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n|disease_name|max_num_claims|\n+------------+--------------+\n| Pet allergy|             3|\n+------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Which disease has a maximum number of claims\n",
    "df1_claims_dupdropped.createOrReplaceTempView(\"df_claims\")\n",
    "df1=spark.sql(\"select disease_name, count(disease_name) as max_num_claims from df_claims group by disease_name order by max_num_claims desc limit 1\")\n",
    "\n",
    "df1.show()\n",
    "df2 = df1.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.disease_claims\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb32fc6-ebd7-4b1e-bdef-861978c2dd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|  subscriber_name|\n+-----------------+\n| BhilanganaPandit|\n|       BandhuSeth|\n|ChandavarmanSingh|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find those Subscribers having age less than 30 and they subscribe any subgroup \n",
    "from pyspark.sql.functions import current_date, datediff, col,round\n",
    "df = df8_subscriber_dupdropped.withColumn(\"age\", round(datediff(current_date(), col(\"Birth_date\")) / 365.25, 1)) #the one here rounds the result to 1 decimal place\n",
    "df.createOrReplaceTempView(\"df1\")\n",
    "#or\n",
    "# df = df8_subscriber_dupdropped.withColumn(\"age\", (year(current_date()) - (year(col(\"Birth_date\")))) #the one here rounds the result to 1 decimal place\n",
    "df.createOrReplaceTempView(\"df1\")\n",
    "df1=spark.sql(\"select concat(first_name,last_name) as subscriber_name,age from df1 where age <30\")\n",
    "#remove show fn in real code\n",
    "df1.show()\n",
    "df2 = df1.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.Sub_les_than_30\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ff39881-2b1f-497e-81f7-d226d460b39e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n|Grp_id|num_subgrp|\n+------+----------+\n|GRP104|         2|\n|GRP147|         2|\n|GRP143|         2|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out which group has maximum subgroups. \n",
    "df4_grpsubgrp_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "df1=spark.sql(\"select Grp_id, count(SubGrp_ID) as num_subgrp from df1 group by Grp_Id\")\n",
    "# Find the maximum count of subgroups\n",
    "max_num_subgrp = df1.agg({\"num_subgrp\": \"max\"}).collect()[0][0] \n",
    "# Filter the DataFrame to include only rows with the maximum count\n",
    "df2 = df1.filter(df1.num_subgrp == max_num_subgrp)\n",
    "\n",
    "#instead TRY RANK\n",
    "\n",
    "# Show the result\n",
    "df2.show()\n",
    "df2 = df2.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.max_subgrp\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d48870-8e62-48b6-840d-c170a9c65fd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+\n|    Hospital_name|most_num_patient|\n+-----------------+----------------+\n|Manipal Hospitals|               9|\n+-----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Find out hospital which serve most number of patients \n",
    "df5_hospital_dupdropped.createOrReplaceTempView(\"df\")\n",
    "df6_patient_record_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "df2=spark.sql(\"select df.Hospital_name,count(df1.Patient_id) as most_num_patient from df join df1 on df.Hospital_id =df1.hospital_id group by df.Hospital_name order by most_num_patient desc limit 1\")\n",
    "df2.show()\n",
    "df2 = df2.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.max_patient\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f8d4ad-9d49-4566-b010-9cb1cc4d9709",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|SubGrp_Name|max_subcriber|\n+-----------+-------------+\n|    Therapy|           13|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Find out which subgroups subscribe most number of times\n",
    "#Note to self: the column name as space in them so, use backtik; eg:count(`df1`.`sub _id`)as max_subcriber\n",
    "df7_subgroup_dupdropped.createOrReplaceTempView(\"df\")\n",
    "df8_subscriber_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "\n",
    "df2=spark.sql(\"select df.SubGrp_Name, count(`df1`.`sub _id`) as max_subcriber from df join df1 on df.SubGrp_id = df1.Subgrp_id group by df.SubGrp_Name order by max_subcriber desc limit 1\")\n",
    "df2.show()\n",
    "df2 = df2.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.most_subcriber\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b9a9f9-84ab-47cc-830d-2ccc7d513f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|Rejected_Claims|\n+---------------+\n|             18|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Find out total number of claims which were rejected\n",
    "df1_claims_dupdropped.createOrReplaceTempView(\"df\")\n",
    "df1= spark.sql(\"select count(claim_id) as Rejected_Claims from df where Claim_Or_rejected == 'Y'\")\n",
    "df1.show()\n",
    "df2 = df1.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.Rejected_Claims\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7cd5a4-2cf3-4d5a-9763-13e2c55d4fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n|        city|max_claim_by_city|\n+------------+-----------------+\n|    Amravati|                2|\n|       Morbi|                2|\n|   Kamarhati|                2|\n|Bihar Sharif|                2|\n|   Ghaziabad|                2|\n|    Jabalpur|                2|\n+------------+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "#From where most claims are coming (city) \n",
    "df = df1_claims_dupdropped.withColumn(\"patient_id\",col(\"patient_id\").cast(\"int\"))\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df6_patient_record_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "\n",
    "df2 = spark.sql(\"\"\"\n",
    "    SELECT df1.city, COUNT(df.claim_id) AS max_claim_by_city\n",
    "    FROM df1\n",
    "    JOIN df ON df1.patient_id = df.patient_id\n",
    "    GROUP BY df1.city\n",
    "    ORDER BY max_claim_by_city DESC\n",
    "    LIMIT 6\n",
    "\"\"\")\n",
    "df2.show()\n",
    "df2 = df2.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.Max_claims_by_city\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4f9e15-5976-46da-bc74-598e69db55f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n|            Grp_Name|Grp_Type|\n+--------------------+--------+\n|Agriculture Insur...|   Govt.|\n|Export Credit Gua...|   Govt.|\n|Life Insurance Co...|   Govt.|\n|National Insuranc...|   Govt.|\n| New India Assurance|   Govt.|\n|The Oriental Insu...|   Govt.|\n|United India Insu...|   Govt.|\n+--------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Which groups of policies subscriber subscribe mostly Government or private\n",
    "df3_group_dupdropped.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df1 =spark.sql(\"select Grp_Name, Grp_Type from df where Grp_Type like 'Gov%' order by Grp_Name \")\n",
    "df1.show()\n",
    "df2 = df1.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.govt_groups\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8a440d-4918-4e88-b2c4-eb4066d0b1a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n|avg_monthly_premium|\n+-------------------+\n|             3200.0|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Average monthly premium subscriber pay to insurance company(note:use AVG fn)\n",
    "df7_subgroup_dupdropped.createOrReplaceTempView(\"df\")\n",
    "df8_subscriber_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "df1 = spark.sql(\"\"\"\n",
    "    SELECT avg(df.Monthly_Premium) AS avg_monthly_premium\n",
    "    FROM df\n",
    "    JOIN df1 ON df.SubGrp_id = df1.Subgrp_id GROUP BY df1.`sub _id` ORDER BY avg_monthly_premium desc limit 1\n",
    "\"\"\")\n",
    "df1.show()\n",
    "df2 = df1.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.Avg_MonthyPremium\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d60f7d6c-cd0a-4846-9ba9-bf4a138b8afd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Find out Which group is most profitable\n",
    "df3_group_dupdropped.createOrReplaceTempView(\"df\")\n",
    "df4_grpsubgrp_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "df7_subgroup_dupdropped.createOrReplaceTempView(\"df2\")\n",
    "\n",
    "\n",
    "df3=spark.sql(\"select df.Grp_Name,df1.SubGrp_ID from df join df1 on df.Grp_Id=df1.Grp_Id\")\n",
    "df3.createOrReplaceTempView(\"df3\")\n",
    "df4=spark.sql(\"select df3.Grp_Name,df2.Monthly_Premium from df3 join df2 on df3.SubGrp_ID=df2.SubGrp_Id order by df2.Monthly_Premium desc limit 1\")\n",
    "df5 = df4.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.Most_Profitable_Grp\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a40458c-6a9e-4e7d-ba7d-2f864f09c830",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n|Patient_name|\n+------------+\n|     Ujjawal|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#List all the patients below age of 18 who admit for cancer\n",
    "#No patient with age less than 18 with cancer so, looking for patient below 45\n",
    "from pyspark.sql.functions import *\n",
    "df6_patient_record_dupdropped.createOrReplaceTempView(\"df\")\n",
    "# Adding a column 'age' to the DataFrame\n",
    "df1 = df6_patient_record_dupdropped.withColumn(\"age\", (datediff(current_date(), col(\"patient_birth_date\")) / 365).cast(\"int\"))\n",
    "# Filtering rows based on age and disease name pattern\n",
    "result_df = df1.filter((col(\"age\") < 45) & ((lower(col(\"disease_name\")).like(\"%can%\")) | (upper(col(\"disease_name\")).like(\"CAN%\")))).select(\"Patient_name\")\n",
    "result_df.show()\n",
    "\n",
    "df2 = df1.write.mode(\"overwrite\").format(\"redshift\").option(\"url\", \"jdbc:redshift://default-workgroup.533267297424.us-east-2.redshift-serverless.amazonaws.com:5439/dev\").\\\n",
    "   option(\"dbtable\", \"test.Cancer_patient_below_45\").\\\n",
    "   option(\"aws_iam_role\", \"arn:aws:iam::533267297424:role/redshiftadmin\").\\\n",
    "   option(\"user\", \"admin\").\\\n",
    "   option(\"tempdir\", \"s3a://puskalawsbucket/tempdir/\").\\\n",
    "   option(\"password\", \"Welcomehome1\").save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5fe545-4e6a-40b1-8678-9a0e04e2d70b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+------------+\n|Claim_Or_Rejected|Patient_name|claim_amount|\n+-----------------+------------+------------+\n|              NaN|       Ekant|      192381|\n|              NaN|    Brahmvir|      192340|\n|              NaN|    Chakrika|      188727|\n|              NaN|          NA|      182552|\n|              NaN|       Aakar|      177186|\n|              NaN|    Gajabahu|      171729|\n|              NaN|          NA|      164159|\n|              NaN|      Gensho|      161199|\n|              NaN|     Paridhi|      159815|\n|              NaN|    Deependu|      156557|\n|              NaN|          NA|      152901|\n|              NaN|    Brahmdev|      151142|\n|              NaN|     Ballari|      143120|\n|              NaN|      Jitesh|      139755|\n|              NaN|      Chanak|      124734|\n|              NaN|          NA|      118452|\n|              NaN|   Madhubala|      108526|\n|              NaN| Bhageeratha|       99313|\n|              NaN|       Swati|       81651|\n|              NaN|  Dharmadaas|       75983|\n+-----------------+------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#List patients who have cashless insurance and have total charges greater than or equal for Rs. 50,000\n",
    "#there are no subgroup with Monthly_premium= 0 so, the question asking for Cashles Insurance is invalid.\n",
    "\n",
    "df = df1_claims_dupdropped.withColumn(\"patient_id\", col(\"patient_id\").cast(\"int\")).withColumn(\"claim_amount\", col(\"claim_amount\").cast(\"int\"))\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "df8_subscriber_dupdropped.createOrReplaceTempView(\"df1\")\n",
    "df7_subgroup_dupdropped.createOrReplaceTempView(\"df2\")\n",
    "df6_patient_record_dupdropped.createOrReplaceTempView(\"df3\")\n",
    "\n",
    "\n",
    "spark.sql(\"select df.Claim_Or_Rejected,  df3.Patient_name,df.claim_amount from df join df3 on df.patient_id = df3.Patient_id where Claim_Or_Rejected like 'Na%' and claim_amount >= 50000 order by claim_amount desc \").show()\n",
    "spark.sql(\"select * from df1 limit 10\").show()\n",
    "spark.sql(\"select * from df2 where `Monthly_Premium`<10 limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20e40e67-5551-471b-9d04-6db3825986a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#List female patients over the age of 40 that have undergone knee surgery in the past year\n",
    "# # No discease name with \"knee Surgery\" found so, No soln for this use case "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "capstone_Notebook_Pyspark_code",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
